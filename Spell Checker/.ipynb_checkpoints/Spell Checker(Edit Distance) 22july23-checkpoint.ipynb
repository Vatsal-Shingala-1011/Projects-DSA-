{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "09e4e511",
   "metadata": {},
   "source": [
    "# Using Levenshtein Distance(Edit Distance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f55af8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This algoritham measures the minimum number of single-character need to edit to make first string to into another string\n",
    "# edit means insertion, deletion, or substitution of char"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bf9cd2b9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\scipy\\__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.25.1\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "************** Spell Checker **************\n",
      "1. Insert Spell\n",
      "2. Check valid Spell or not\n",
      "3. Remove Contact\n",
      "4. Display Dictionary\n",
      "5. Exit\n",
      "Enter your choice: 1\n",
      "Enter word: f\n",
      "\n",
      "************** Spell Checker **************\n",
      "1. Insert Spell\n",
      "2. Check valid Spell or not\n",
      "3. Remove Contact\n",
      "4. Display Dictionary\n",
      "5. Exit\n",
      "Enter your choice: 5\n",
      "Exiting Spell Checker System. Goodbye!\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import nltk\n",
    "from nltk.corpus import words\n",
    "\n",
    "class Color:\n",
    "    RED = 0\n",
    "    BLACK = 1\n",
    "\n",
    "class Node:\n",
    "    def __init__(self, word):\n",
    "        self.word = word\n",
    "        self.color = Color.RED  # by default, the color is red\n",
    "        self.parent = None\n",
    "        self.left = None\n",
    "        self.right = None\n",
    "\n",
    "class SpellChecker:\n",
    "    def __init__(self):\n",
    "        self.root = None\n",
    "        self.TNULL = Node(\"\")  # NULL pointer\n",
    "        self.TNULL.color = Color.BLACK\n",
    "        self.TNULL.left = None\n",
    "        self.TNULL.right = None\n",
    "        self.root = self.TNULL\n",
    "\n",
    "    def create_node(self, word):\n",
    "        node = Node(word)\n",
    "        node.left = self.TNULL\n",
    "        node.right = self.TNULL\n",
    "        return node\n",
    "\n",
    "    def left_rotate(self, x):\n",
    "        y = x.right\n",
    "        x.right = y.left\n",
    "        if y.left != self.TNULL:\n",
    "            y.left.parent = x\n",
    "        y.parent = x.parent\n",
    "        if x.parent is None:\n",
    "            self.root = y\n",
    "        elif x == x.parent.left:\n",
    "            x.parent.left = y\n",
    "        else:\n",
    "            x.parent.right = y\n",
    "        y.left = x\n",
    "        x.parent = y\n",
    "\n",
    "    def right_rotate(self, y):\n",
    "        x = y.left\n",
    "        y.left = x.right\n",
    "        if x.right != self.TNULL:\n",
    "            x.right.parent = y\n",
    "        x.parent = y.parent\n",
    "        if y.parent is None:\n",
    "            self.root = x\n",
    "        elif y == y.parent.left:\n",
    "            y.parent.left = x\n",
    "        else:\n",
    "            y.parent.right = x\n",
    "        x.right = y\n",
    "        y.parent = x\n",
    "\n",
    "    def make_red_black_tree_insert(self, k): #function for make red black tree proparties\n",
    "        while k.parent is not None and k.parent.color == Color.RED:\n",
    "            grandparent = k.parent.parent\n",
    "            if k.parent == grandparent.right: #k->parent's sibling\n",
    "                uncle = grandparent.left   #if uncle color is red then tree is balance no need of rotation\n",
    "                if uncle.color == Color.RED:\n",
    "                    uncle.color = Color.BLACK\n",
    "                    k.parent.color = Color.BLACK\n",
    "                    grandparent.color = Color.RED\n",
    "                    k = grandparent\n",
    "                else:  #uncle->color == Black\n",
    "                    if k == k.parent.left:\n",
    "                        k = k.parent\n",
    "                        self.right_rotate(k)\n",
    "                    k.parent.color = Color.BLACK\n",
    "                    grandparent.color = Color.RED\n",
    "                    self.left_rotate(grandparent)\n",
    "            else:\n",
    "                uncle = grandparent.right\n",
    "                if uncle.color == Color.RED:\n",
    "                    uncle.color = Color.BLACK\n",
    "                    k.parent.color = Color.BLACK\n",
    "                    grandparent.color = Color.RED\n",
    "                    k = grandparent\n",
    "                else:\n",
    "                    if k == k.parent.right:\n",
    "                        k = k.parent\n",
    "                        self.left_rotate(k)\n",
    "                    k.parent.color = Color.BLACK\n",
    "                    grandparent.color = Color.RED\n",
    "                    self.right_rotate(grandparent)\n",
    "            if k == self.root:\n",
    "                break\n",
    "        self.root.color = Color.BLACK\n",
    "\n",
    "    def insert(self, word): #insert contact at correct possition and satisfy red black tree proparties\n",
    "        prev = None\n",
    "        node = self.root\n",
    "        while node != self.TNULL:\n",
    "            prev = node\n",
    "            if word < node.word:\n",
    "                node = node.left\n",
    "            elif word > node.word:\n",
    "                node = node.right\n",
    "            else:\n",
    "                # If the word is already in the dictionary, return\n",
    "                return\n",
    "        new_node = self.create_node(word)\n",
    "        new_node.parent = prev\n",
    "        if prev is None:\n",
    "            self.root = new_node\n",
    "        elif word < prev.word:\n",
    "            prev.left = new_node\n",
    "        else:\n",
    "            prev.right = new_node\n",
    "        self.make_red_black_tree_insert(new_node)\n",
    "\n",
    "    def find_node(self, word, node):\n",
    "        if node == self.TNULL or node.word == word:\n",
    "            return node\n",
    "        if word < node.word:\n",
    "            return self.find_node(word, node.left)\n",
    "        return self.find_node(word, node.right)\n",
    "\n",
    "    def find_min_node(self, node):\n",
    "        while node.left != self.TNULL:\n",
    "            node = node.left\n",
    "        return node\n",
    "\n",
    "    def make_red_black_tree_remove(self, x): #make tree redblacktree after removing contact\n",
    "        while x != self.root and x.color == Color.BLACK:\n",
    "            if x == x.parent.left:\n",
    "                s = x.parent.right\n",
    "                if s.color == Color.RED:\n",
    "                    s.color = Color.BLACK\n",
    "                    x.parent.color = Color.RED\n",
    "                    self.left_rotate(x.parent)\n",
    "                    s = x.parent.right\n",
    "\n",
    "                if s.left.color == Color.BLACK and s.right.color == Color.BLACK:\n",
    "                    s.color = Color.RED\n",
    "                    x = x.parent\n",
    "                else:\n",
    "                    if s.right.color == Color.BLACK:\n",
    "                        s.left.color = Color.BLACK\n",
    "                        s.color = Color.RED\n",
    "                        self.right_rotate(s)\n",
    "                        s = x.parent.right\n",
    "\n",
    "                    s.color = x.parent.color\n",
    "                    x.parent.color = Color.BLACK\n",
    "                    s.right.color = Color.BLACK\n",
    "                    self.left_rotate(x.parent)\n",
    "                    x = self.root\n",
    "            else:\n",
    "                s = x.parent.left\n",
    "                if s.color == Color.RED:\n",
    "                    s.color = Color.BLACK\n",
    "                    x.parent.color = Color.RED\n",
    "                    self.right_rotate(x.parent)\n",
    "                    s = x.parent.left\n",
    "\n",
    "                if s.right.color == Color.BLACK and s.left.color == Color.BLACK:\n",
    "                    s.color = Color.RED\n",
    "                    x = x.parent\n",
    "                else:\n",
    "                    if s.left.color == Color.BLACK:\n",
    "                        s.right.color = Color.BLACK\n",
    "                        s.color = Color.RED\n",
    "                        self.left_rotate(s)\n",
    "                        s = x.parent.left\n",
    "\n",
    "                    s.color = x.parent.color\n",
    "                    x.parent.color = Color.BLACK\n",
    "                    s.left.color = Color.BLACK\n",
    "                    self.right_rotate(x.parent)\n",
    "                    x = self.root\n",
    "\n",
    "        x.color = Color.BLACK\n",
    "\n",
    "    def rb_transplant(self, u, v):\n",
    "        if u.parent is None:\n",
    "            self.root = v\n",
    "        elif u == u.parent.left:\n",
    "            u.parent.left = v\n",
    "        else:\n",
    "            u.parent.right = v\n",
    "        v.parent = u.parent\n",
    "\n",
    "    def remove_word(self, word):\n",
    "        z = self.find_node(word, self.root)\n",
    "        if z == self.TNULL:\n",
    "            print(\"Spell not found in the dictionary.\",end=\"\")\n",
    "            return\n",
    "        x = None\n",
    "        y = z\n",
    "        original_color = y.color\n",
    "        if z.left == self.TNULL:\n",
    "            x = z.right\n",
    "            self.rb_transplant(z, z.right)\n",
    "        elif z.right == self.TNULL:\n",
    "            x = z.left\n",
    "            self.rb_transplant(z, z.left)\n",
    "        else:\n",
    "            y = self.find_min_node(z.right)\n",
    "            original_color = y.color\n",
    "            x = y.right\n",
    "            if y.parent == z:\n",
    "                x.parent = y\n",
    "            else:\n",
    "                self.rb_transplant(y, y.right)\n",
    "                y.right = z.right\n",
    "                y.right.parent = y\n",
    "            self.rb_transplant(z, y)\n",
    "            y.left = z.left\n",
    "            y.left.parent = y\n",
    "            y.color = z.color\n",
    "        del z\n",
    "        if original_color == Color.BLACK:\n",
    "            self.make_red_black_tree_remove(x)\n",
    "        print(\"Spell removed successfully.\")\n",
    "\n",
    "    def inorder_traversal(self, node):\n",
    "        if node != self.TNULL:\n",
    "            self.inorder_traversal(node.left)\n",
    "            print(node.word + \", \", end=\"\")\n",
    "            self.inorder_traversal(node.right)\n",
    "       \n",
    "    def levenshtein_distance(self,s1, s2):\n",
    "        m, n = len(s1), len(s2)\n",
    "        dp = np.zeros((m + 1, n + 1), dtype=int)\n",
    "        for i in range(m + 1):\n",
    "            dp[i][0] = i\n",
    "        for j in range(n + 1):\n",
    "            dp[0][j] = j\n",
    "        for i in range(1, m + 1):\n",
    "            for j in range(1, n + 1):\n",
    "                cost = 0 if s1[i - 1] == s2[j - 1] else 1\n",
    "                dp[i][j] = min(dp[i - 1][j] + 1, dp[i][j - 1] + 1, dp[i - 1][j - 1] + cost)\n",
    "        return dp[m][n]\n",
    "    \n",
    "    # Function to find suggestions for a given word\n",
    "    def get_suggestions(self,word, dictionary, max_distance=1):\n",
    "        suggestions = []\n",
    "        suggestions2 = []\n",
    "        for dict_word in dictionary:\n",
    "            distance = self.levenshtein_distance(word, dict_word)\n",
    "            if distance <= max_distance:\n",
    "                suggestions.append(dict_word)\n",
    "            if distance<=max_distance+1:\n",
    "                suggestions2.append(dict_word)\n",
    "        if suggestions==[]:\n",
    "            return suggestions2\n",
    "        return suggestions\n",
    "    \n",
    "    def search(self,word):\n",
    "        node=self.find_node(word,self.root) #return none if not found\n",
    "        if node is not self.TNULL:\n",
    "            print(\" word is spelled correctly\",end=\"\")\n",
    "        else:\n",
    "            print(\"word \\\"\",word,\"\\\" is not correct. possible spelling may be : \",end=\"\")\n",
    "            suggestions = self.get_suggestions(word, dictionary,1)\n",
    "            if suggestions: #suggestions is not empty\n",
    "                print(\", \".join(suggestions))\n",
    "            else: #could not find any word similar to given word\n",
    "                print(\"could not find any word similar to \\\"\", word,\"\\\"\",end=\"\")  \n",
    "            return \n",
    "    \n",
    "if __name__ == \"__main__\":\n",
    "#     dictionary = [\"apple\", \"banana\", \"orange\", \"pear\", \"peach\", \"grape\", \"watermelon\"]\n",
    "    dictionary = words.words()\n",
    "    S = SpellChecker()\n",
    "    for word in dictionary:\n",
    "        S.insert(word)\n",
    "\n",
    "    choice = None\n",
    "    while choice != \"5\":\n",
    "        print(\"\\n************** Spell Checker **************\")\n",
    "        print(\"1. Insert Spell\")\n",
    "        print(\"2. Check valid Spell or not\")\n",
    "        print(\"3. Remove Contact\")\n",
    "        print(\"4. Display Dictionary\")\n",
    "        print(\"5. Exit\")\n",
    "        choice = input(\"Enter your choice: \")\n",
    "        if choice == \"1\":\n",
    "            word = input(\"Enter word: \")\n",
    "            S.insert(word)#insert in Tree\n",
    "            dictionary.append(word)\n",
    "        elif choice == \"2\":\n",
    "            word = input(\"Enter word: \")\n",
    "            node = S.search(word)\n",
    "        elif choice == \"3\":\n",
    "            word = input(\"Enter word: \")\n",
    "            S.remove_word(word)\n",
    "        elif choice == \"4\":\n",
    "            print(\"********** All Words in the dictionary **********\")\n",
    "            S.inorder_traversal(S.root)\n",
    "        elif choice == \"5\":\n",
    "            print(\"Exiting Spell Checker System. Goodbye!\\n\")\n",
    "        else:\n",
    "            print(\"Invalid choice. Please try again.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c46c8c81",
   "metadata": {},
   "source": [
    "# Using Transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "262a109f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install tensorflow tensorflow-hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3c7854e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install tensorflow==1.15.0\n",
    "# !pip install bert-tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50378196",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now, the code should run without any errors. It will use the pre-trained BERT model specified by model_name to calculate word embeddings and find the most similar word from the sample dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7e99d114",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "27be7946401b49f7bba8621b26c2c490",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)solve/main/vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\vatsal shingala\\AppData\\Roaming\\Python\\Python39\\site-packages\\huggingface_hub\\file_download.py:133: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\vatsal shingala\\.cache\\huggingface\\hub. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to see activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f262bbd06a894d7c94930d56b7df4146",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)okenizer_config.json:   0%|          | 0.00/28.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1f08302f828c4074b3e6e2ba8b538508",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)lve/main/config.json:   0%|          | 0.00/571 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4b0b36591beb4b5996e94b2073cbf4cf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading model.safetensors:   0%|          | 0.00/1.34G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFBertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias']\n",
      "- This IS expected if you are initializing TFBertModel from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFBertModel from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the weights of TFBertModel were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'bert_params' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[1;32mIn [6]\u001b[0m, in \u001b[0;36m<cell line: 44>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     44\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m     45\u001b[0m     text \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mI love naural langage pocessing.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m---> 46\u001b[0m     suggested_word \u001b[38;5;241m=\u001b[39m \u001b[43mspell_check\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     47\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOriginal Text:\u001b[39m\u001b[38;5;124m\"\u001b[39m, text)\n\u001b[0;32m     48\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSuggested Word:\u001b[39m\u001b[38;5;124m\"\u001b[39m, suggested_word)\n",
      "Input \u001b[1;32mIn [6]\u001b[0m, in \u001b[0;36mspell_check\u001b[1;34m(text)\u001b[0m\n\u001b[0;32m     32\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mspell_check\u001b[39m(text):\n\u001b[1;32m---> 33\u001b[0m     avg_embedding \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mmean(\u001b[43mget_bert_embedding\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m)\u001b[49m, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m     35\u001b[0m     \u001b[38;5;66;03m# Sample dictionary for demonstration purposes\u001b[39;00m\n\u001b[0;32m     36\u001b[0m     dictionary \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnatural\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlanguage\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mprocessing\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "Input \u001b[1;32mIn [6]\u001b[0m, in \u001b[0;36mget_bert_embedding\u001b[1;34m(text)\u001b[0m\n\u001b[0;32m     13\u001b[0m tokens \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m[CLS]\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m+\u001b[39m tokenizer\u001b[38;5;241m.\u001b[39mtokenize(text) \u001b[38;5;241m+\u001b[39m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m[SEP]\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m     14\u001b[0m token_ids \u001b[38;5;241m=\u001b[39m tokenizer\u001b[38;5;241m.\u001b[39mconvert_tokens_to_ids(tokens)\n\u001b[1;32m---> 15\u001b[0m token_ids \u001b[38;5;241m=\u001b[39m token_ids \u001b[38;5;241m+\u001b[39m [\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m*\u001b[39m (\u001b[43mbert_params\u001b[49m\u001b[38;5;241m.\u001b[39mmax_seq_length \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mlen\u001b[39m(token_ids))\n\u001b[0;32m     17\u001b[0m input_ids \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray([token_ids])\n\u001b[0;32m     18\u001b[0m segment_ids \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mzeros_like(input_ids)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'bert_params' is not defined"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from nltk.tokenize import word_tokenize\n",
    "from transformers import BertTokenizer, TFBertModel\n",
    "import bert\n",
    "\n",
    "# Replace 'bert-base-uncased' with the name of the pre-trained BERT model you want to use\n",
    "model_name = 'bert-large-uncased' #you can choose from a variety of pre-trained BERT models available in the Hugging Face model hub, such as 'bert-large-uncased', 'bert-base-cased', 'bert-large-cased', and more.\n",
    "tokenizer = BertTokenizer.from_pretrained(model_name)\n",
    "model = TFBertModel.from_pretrained(model_name)\n",
    "\n",
    "def get_bert_embedding(text):\n",
    "    tokens = [\"[CLS]\"] + tokenizer.tokenize(text) + [\"[SEP]\"]\n",
    "    token_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "    token_ids = token_ids + [0] * (bert_params.max_seq_length - len(token_ids))\n",
    "\n",
    "    input_ids = np.array([token_ids])\n",
    "    segment_ids = np.zeros_like(input_ids)\n",
    "\n",
    "    with tf.Graph().as_default():\n",
    "        l_input_ids = tf.keras.layers.Input(shape=(bert_params.max_seq_length,), dtype='int32')\n",
    "        l_segment_ids = tf.keras.layers.Input(shape=(bert_params.max_seq_length,), dtype='int32')\n",
    "\n",
    "        output = l_bert([l_input_ids, l_segment_ids])\n",
    "        model = tf.keras.models.Model(inputs=[l_input_ids, l_segment_ids], outputs=output)\n",
    "\n",
    "        model.load_weights(bert_path)\n",
    "        embeddings = model.predict([input_ids, segment_ids])\n",
    "\n",
    "    return embeddings[0][0]\n",
    "\n",
    "def spell_check(text):\n",
    "    avg_embedding = np.mean(get_bert_embedding(text), axis=0)\n",
    "\n",
    "    # Sample dictionary for demonstration purposes\n",
    "    dictionary = [\"natural\", \"language\", \"processing\"]\n",
    "\n",
    "    similarity_scores = [np.dot(avg_embedding, get_bert_embedding(word)) for word in dictionary]\n",
    "    suggested_index = np.argmax(similarity_scores)\n",
    "    suggested_word = dictionary[suggested_index]\n",
    "\n",
    "    return suggested_word\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    text = \"I love naural langage pocessing.\"\n",
    "    suggested_word = spell_check(text)\n",
    "    print(\"Original Text:\", text)\n",
    "    print(\"Suggested Word:\", suggested_word)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea0a1ecb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
